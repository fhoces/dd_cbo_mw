---
title: "Reader Companion for CBO report on Min Wage"
author: "Fernando Hoces de la Guardia"
date: '`r Sys.Date()`'
output:
  html_document:
    code_folding: hide
    collapsed: no
    number_sections: yes
    smooth_scroll: no
    theme: united
    toc: yes
    toc_depth: 2
    toc_float: no
  pdf_document:
    toc: yes
    toc_depth: '2'
---


<!-- 
resource_files:
- apps/app1/app.R
runtime: shiny
-->

```{r setup, include=FALSE}
if (TRUE) {
  if (Sys.info()["sysname"] == "Darwin") {
    setwd("/Users/fhocesde/Documents/dissertation/Replication")
  }else{
    setwd("C:/Users/fhocesde/Documents/replication")
  }
}

# Loading required libraries
list.of.packages <- c("knitr","foreign", "dplyr", "weights", "survey", "Hmisc", 
                      "openxlsx", "rio", "highr", "XML", "RCurl", "treemap", 
                      "reshape2", "tidyr")

new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages, repos= "http://cran.cnr.berkeley.edu/") 

lapply(list.of.packages, require, character.only = TRUE)
# Setting working directory

knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(cache = TRUE)
```


# Introduction  
This reader companion presents the key/analytic components of the CBO report on the effect of raising the minimum wage in a transparent and reproducible template. Our purpose is to describe every step/decision involved in this policy analysis such that analysts/stakeholder can understand/critize/improve from it.  

The new reproducibility practices promoted in science were used as a framework to carry out the excercise described below. However we need to highlight a crucial difference between a replication in science and a replication in policy analysis. A scientific report takes the form of a peer review publication that represent  several months or years of research, followed up by a review process that can be as lengthy as the research itself. Because of this when a scientific publication is subject to replication is expected to succedd. Policy analysis is ususally performed under tight deadlines, and is not unusual to rely on arbitrary assumptions and/or unreproducible calculations. For this reasons we do not attempt to replicate the CBO report as a way of testing the veracity of the analysis. We use reproducibility, paired with full transparency, to generate a living document that represents the best policy analysis up to date. Our expectations are that this living document will be serve as a building block to discuss and incorporate incremental improvements on the best available policy analysis. 

The CBO report describes two policy estimates: the effects of minimum wage on employment and the distributional effects of minimum wage on wages and familiy income. All the policy estimates to replicate are presented in the following tables. 

```{r table with output,  eval=TRUE,echo=FALSE, warning=FALSE, message=FALSE, cache=TRUE}  
# This is summary of all the policy estimates presented in the report
# Aggregate effects
output.template1 <- matrix(" ???",nrow = 7, ncol = 1)
rownames(output.template1) <- c("wage gains", "wage losses", "Balance losses", 
                                "Net effect", "# of Wage gainers", "#of Wage losers", 
                                "Population")
colnames(output.template1) <- "Effects/Policy Estimates"

output.template1[,"Effects/Policy Estimates" ] <- c("31", "~5", "~24" , "2", "16.5", "0.5", "330/140")


#Some distributional effects
output.template2 <- matrix(" ???",nrow = 2, ncol = 5)
rownames(output.template2) <- c("Balance losses", "Net effect")
colnames(output.template2) <- c("<1PL", "[1PL, 3PL)", "[3PL, 6PL)", ">6PL", "Total")

output.template2["Balance losses", ] <- c("~0.3", "~3.4", "~3.4", "~17", "~24")
output.template2["Net effect", ] <- c("5", "12", "2", "-17", "2")



knitr::kable(
  list(
    output.template1,
    output.template2
  ),
  caption = 'Policy estimates in CBO report', booktabs = TRUE, 
  align = 'c'
)


#This is the first suggestion: change how policy estimates are reported and add information.
mod.output.template <- matrix(" ???",nrow = 7, ncol = 5)
rownames(mod.output.template) <- c("wage gains", "wage losses", "Balance losses", "Net effect", "# of Wage gainers", "#of Wage losers", "Population")
colnames(mod.output.template) <- c("<1PL", "[1PL, 3PL)", "[3PL, 6PL)", ">6PL", "Total")

mod.output.template[,"Total" ] <- c("31", "~5", "~24" , "2", "16.5", "0.5", "330/140")
mod.output.template["Balance losses", ] <- c("~0.3", "~3.4", "~3.4", "~17", "~24")
mod.output.template["Net effect", ] <- c("5", "12", "2", "-17", "2")


#knitr::kable(mod.output.template, caption="Template for final results to replicate", digits = 1)
```    

This companion presents the details behind both policy estimates .   

# Employment effects
At a general level the effects on employment ($\widehat{\Delta E}$) will be calculated using a more detailed version of the following equation:

$$
\begin{align}
\widehat{\Delta E} &= N \times \eta \times \% \Delta w  + \text{Other factors}
\end{align}
$$
  
Where $N$ represents the relevant population, $\eta$ the elasticity of labor demand, $\Delta w$ the relevant percentual variation in wages, and the *Other factors* will encapsulate effects on employment throught an increase in the aggreagate demand.  

To describe the methodology behind each of those four components we first describe the data used, the wage variable choosen, and the procedure used to forecast the wage and population distribution of 2016 using data from 2013.  


## Data, wages, and forecast  
To simulate the policy effects we need the distribution of wages and employment under the status quo. From the perspective of 2013, this implies forecasting to 2016 data on employment and wages. 


### Data
The Current Population Survey (CPS) was used to compute the effects on employment. From the analysis in the section on distributional effects we can deduce that the data corresponds to the Outgoing Rotation Group (ORG). CPS is a monthly cross sectional suvey. The same individual is interviewed eight times over a period of 12 months. The interviews take place in the first and las 4 months of that period. By the 4th and 12th interview, individuals are asked detailed information on earnings. The CPS ORG file contains the information on this interviews for a given year. We analyze the data for 2013.   

Currently three versions of these data sets can be found online: [CPS raw files](http://thedataweb.rm.census.gov/ftp/cps_ftp.html#cpsbasic), [ORG NBER](http://www.nber.org/data/morg.html) and [ORG CEPR](http://ceprdata.org/cps-uniform-data-extracts/cps-outgoing-rotation-group/). The analysis will be performed using the CPER ORG data base.

The weights used in our analysis will be `orgwgt/12`

#### Code to load the data  
```{r loading data, eval=TRUE,echo=TRUE, warning=FALSE, message=FALSE}

data_use <- "CPER_ORG"

# Using CEPR ORG data 
if (data_use == "CPER_ORG") {
# Checking if working directory contains data, download if not. 
  if ( !("cepr_org_2013.dta" %in% dir()) ) {
  	# create name of file to store data
  	tf <- "cepr_org_2013.zip"
  
  	# download the CPS repwgts zipped file to the local computer
  	download.file(url =  "http://ceprdata.org/wp-content/cps/data/cepr_org_2013.zip", tf , mode = "wb" )
  
  	# unzip the file's contents and store the file name within the temporary directory
  	fn <- unzip( zipfile = tf , overwrite = T )
  }
  df <- read.dta("cepr_org_2013.dta")
}

# Using NBER ORG data 
if (data_use == "NBER_ORG") {
  # Checking if working directory contains data, download if not. 
  if ( !("morg13.dta" %in% dir()) ) {
    # Downloading data 53mb
    df <- read.dta("http://www.nber.org/morg/annual/morg13.dta")
  }
  df <- read.dta("morg13.dta")
}

df <- tbl_df(df)

# There are 1293 cases with missin values for the weigths. I delete them from the data. 
df <- df %>% filter(!is.na(orgwgt))
df$final_weights <- df$orgwgt/12
```  

### Wage variable
We assume no further adjustments like imputation for top coding, trimming, excluding overtime/commissions, or imputation of usual hours for ''hours vary'' respondents. The CEPR ORG data includes several wage variables ([described here](http://ceprdata.org/cps-uniform-data-extracts/cps-outgoing-rotation-group/)). The wage variable that best matches the description above is `wage3`

### Wage adjustment
An adjustment was made to the wage of all the wokers that did not report an hourly wage (`wage3` is estimated as usual salary per self-reported pay-periodver usual hours per pay-period). In order to reduce the measurement error in those wages, we follow the methodology proposed in [this paper](https://www.aeaweb.org/articles?id=10.1257/aer.96.3.461) and compute the adjusted wage as a weigted average of the original wage and the average wage of workers with similar characteristics. 


$$
\begin{align}
w_{ig} &= \alpha w^{raw}_{ig} - (1 - \alpha)  \overline{w^{raw}_{g}} \label{samp.eq1} \\
\text{with:    } \quad \overline{w^{raw}_{g}} &= \frac{\sum_{g} w^{raw}_{ig} }{N_{g}}  \nonumber 
\end{align}
$$ 
  
TO BE COMPLETED: Ask CBO about $\alpha$ and $G$.    


### Wage forecast  
With this data-variable-adjustment we forecast the wage distribution, from 2013 to 2016 in the following way:

#### Growth adjustments
We assume that the growth forecasts were taken from the 10-Year Economic Projections from CBO ([this website](https://www.cbo.gov/about/products/budget_economic_data)). Annualized growth rates for the number of workers $g_{workers}$, and nominal wage per $g_{wages}$ worker where computed as follows:  

$$
\begin{align}
\widehat{ g_{workers} } &= \left[ \frac{\widehat{ N_{workers}^{2016} } }{N_{workers}^{2013}} \right]^{1/3}- 1 \\
\widehat{ g_{wages} }  &= \left[ \frac{\widehat{ Wages^{2016} } / \widehat{ N_{workers}^{2016} } }{Wages^{2013} / N_{workers}^{2013}} \right]^{1/3} - 1 
\end{align}
$$ 

The report assumes higher wage growth for hich wages than low wages. To create different rates of growth in wage, we compute different wage growth rates for each decile of wage. The increments across deciles were constant and the set to match a final lowest decile with a yearly growth rate of 2.9%.  

The adjustment over number of workers was made through the weight variable `final_weights` (multiplying it by the growth rate) whereas the `wage3` variable was multiplied by the forecasted growth rate of per worker wages.   

##### Code to get economic growth forecasts    

```{r Getting forecast data, eval=TRUE,echo=TRUE, warning=FALSE, message=FALSE, results = "hide"}
# All projections data comes from this website: https://www.cbo.gov/about/products/budget_economic_data
# name of the files that contain projections from CBO
early.2016  <- "51135-2016-01-Economic%20Projections.xlsx" 
late.2015   <- "51135-2015-08-EconomicProjections.xlsx"
early.2015  <- "51135-2015-01-EconomicProjections.xlsx" 
late.2014   <- "51135-2014-08-EconomicProjections.xlsx"
early.2014  <- "51135-2014-02-EconomicProjections.xlsx"
early.2013  <- "51135-2013-02-EconomicProjections.xls"  #there is no late 2013 report

# This function loads the data for a given report
get.growth.data <- function(x){
# Checking if working directory contains data, download if not. 
  if ( !(x %in% dir()) ) {
    download.file(url = paste("https://www.cbo.gov/sites/default/files/", 
                              x , sep = ""), 
                  destfile = x, mode="wb")
  }
  if (x == early.2013) {
    if  ( !(require(XLConnect)) ) install.packages("XLConnect")
      out.df <- rio::import( x , sheet= "2. Calendar Year")
  } else {
      out.df <- read.xlsx( x , sheet = "2. Calendar Year")
  }
  return(out.df)
}

# Working with projections from 2013
trends.df <- get.growth.data( early.2013 )

# Get column of all projections for 2013
sel.col <- which(trends.df==2012, arr.ind = TRUE)[2] 
# Get row with all projections for wages and salaries in billions of (nominal) dollars
# Note: the excel file always contains two rows with the words "wage[s]" and "salar[ies|y]", 
# we are looking for the second one (corresponding to Wages and Salaries under Income)
sel.row1 <- unique(
                    apply(trends.df,
                          2, function(x)  grep("Wage.*Salar.*",x ) )
                    )[[2]]
sel.row1 <- sel.row1[2]

# Get row with all projections for number people employed (in millions)
sel.row2 <- which(trends.df=="Employment, Civilian, 16 Years or Older (Household Survey)", arr.ind = TRUE)[1]
# FH: I would use the following. But CBO uses the Price Index, Personal Consumption Expenditures (PCE)
# sel.row3 <- unique(
#                     apply(trends.df,
#                           2, function(x)  grep("Nonwage Income", x ) )
#                   )[[2]]
# 
#

sel.row3 <- unique(
                    apply(trends.df,
                          2, function(x)  grep("Price Index, Personal Consumption", x ) )
                  )[[2]]


#Keep only rows and colums identified above
trends.df <- trends.df[c(sel.row1, sel.row2, sel.row3) , sel.col:(sel.col+7)]

#Labeling and formating
colnames(trends.df) <- 2012:(2012+7)
trends.df <- apply(trends.df, 2, as.numeric)
row.names(trends.df) <- c( "wages(total)", "workers", "Price Index, Personal Consumption")

#Generate wage and non-wage income per worker
trends.df <- rbind( trends.df ,  
                    (trends.df["wages(total)", ] * 1e9 ) / ( trends.df["workers", ] * 1e6) )

row.names(trends.df) <- c( "wages(total)", "workers", "Price Index, Personal Consumption",
                            "wages per worker")

#Transpose the data
trends.df <- t(trends.df)

# Define a new data set with the anual growth rate of each variable over time
growth.df <- trends.df/lag(trends.df,1) - 1 

# Compute the compounded growth factor for a given variable in a time interval
# For example growth factor between years 1,2 and 3 will be:
# (1+growth_rate_yr1) * (1+growth_rate_yr2) * (1+growth_rate_yr3)
gr.factor <- function(var1, init.year, last.year) {
  if (init.year == 2012) {init.year <- 2013}
  prod((growth.df[, var1 ] + 1)[as.character(init.year:last.year)])
} 
``` 

#### ACA adjustments  

[Not done yet]  

#### State level minimum wage adjustments   

CBO had to predict the future changes in the state level minimum wages. We use the actual values implemented by each state. The data comes from the Department of Labor ([here](https://www.dol.gov/whd/state/stateMinWageHis.htm)). 

Whenever the predicted wages were below the 2016 state minimum wage they were replace by it. 

**Important assumption:** when imputing state level min wages, we assume that no effects on employment where incorporated. 

##### Code to get minimum wage values by state  
```{r Getting min wage data , eval=TRUE,echo=TRUE, warning=FALSE, message=FALSE, results = "hide"}
# Minimum wage by state: 
# Check if data is in machine and download if not.
# To excecute the following piece of code you cannot be behind a firewall
if ( !("minwage" %in% dir()) ) {
  fileURL <- "https://www.dol.gov/whd/state/stateMinWageHis.htm"
  xData <- getURL(fileURL)
  aux.1 <- readHTMLTable(xData, header = TRUE)
  
  min.wage.data <- cbind(aux.1[[1]], aux.1[[2]][,-1], 
                         aux.1[[3]][,-1], aux.1[[4]][,-1], 
                         aux.1[[5]][1:55,-1])
  min.wage.data <- min.wage.data[, - (32:37)]
  colnames(min.wage.data) <- c(gsub("(.*)([0-9]{4})(.*)","\\2",
                                    names(min.wage.data))[-c(30, 31)], 
                               "2014", "2015")
  rownames(min.wage.data) <- min.wage.data[,1]
  min.wage.data <- min.wage.data[,-1]

  # This part was hard coded, important to check over and over. 
  rownames(min.wage.data) <- c("Federal","AK","AL","AR","AZ","CA","CO","CT",
                            "DE","FL","GA","HI","ID","IL","IN","IA","KS","KY",
                            "LA","ME","MD","MA","MI","MN","MS","MO",
                            "MT","NE","NV","NH","NJ","NM","NY","NC",
                            "ND","OH","OK","OR","PA","RI","SC","SD",
                            "TN","TX","UT","VT","VA","WA","WV","WI",
                            "WY","DC", "Guam", "PR", "USVI")
  
  #Save all min wage data in a single csv file
  saveRDS(min.wage.data, "minwage")
}

min.wage.data <- readRDS("minwage")


# Function that extracts (in numeric format) the min wage for a specfic year for each state
state.minw <- function(char.year) {
  options(warn=-1)
  aux.1 <- as.numeric(gsub("(.*)([0-9.]{5})(.*)", 
                          "\\2", min.wage.data[, char.year]))
  # If no state min wage, assign federal.
  res1 <- as.data.frame(ifelse(is.na(aux.1), aux.1[1] , aux.1))
  options(warn=0)
  rownames(res1) <- rownames(min.wage.data) 
  colnames(res1) <- char.year 
  return(res1)
}

st.minw <- state.minw("2013")

#CBO makes a forecast of future min wages. We can look at the actual min wage that took place. 
#If CBO provides their forecast, we could check forecast acuracy. 
#Min wage from 2016 are not available in the website above. I am hard coding any changes
#found in wikipedia (https://en.wikipedia.org/wiki/Minimum_wage_in_the_United_States access 5/16/2016):

st.minw.2016 <- state.minw("2015")
st.minw.2016[c("AK"	, "AZ", "CA", "CT"	, "HI"	, "IL", "MA", "MI"	, 
       "MN"	, "MT" , "NV" , "NE" , "NY"	, "OH"	, "RI", "VT"), ] <- 
          c(  9.75	, 8.05, 10	, 9.6	  , 8.5	  , 8.25, 10	, 8.5	  , 
      9		  , 8.05 , 8.25 , 9		 , 9		, 8.1	  , 9.6 , 9.6)
colnames(st.minw.2016) <- "2016"
```

#### Code to forecast wages and workers
```{r Adjusting wages to 2016 , eval=TRUE,echo=TRUE, warning=FALSE, message=FALSE, results = "hide"}
#Wage adjutsment

#CBO mentions that the lowest 10th percent gets a 2.9% growth in anual wage
#I compute the anualized growth rate of wages and creat 10 bins of wage growth
#starting at 2.4%, then adjust by minimum wages of 2016 and get a anualized 
#growth of 2.9% for the lowest decile. 
wage.gr <- ( gr.factor("wages per worker", 2014, 2016) )^(1/3) - 1

workers.gr <- ( gr.factor("workers", 2014, 2016) )^(1/3) - 1

half.gap <- wage.gr - 0.024
wage.gr.bins <- seq(0.024, wage.gr + half.gap, length.out = 10)

# CAUTION: DO NOT apply 'ntile()' fn from dplry as is will split ties differently than 'cut()' and results will no
# be comparable to STATA. 
aux.var  <- wtd.quantile(x = df$wage3, probs = 1:9/10,weights = df$final_weights)
df <- df %>%
        mutate( w3.deciles = cut(wage3, c(0, aux.var, Inf) , 
                                right = TRUE, include.lowest = TRUE) ,  
                w3.adj1 =  wage3 * ( 1 + wage.gr.bins[w3.deciles] )^3) 


# Here we adjust min wages
df$w3.adj.min <- with( df, ifelse(w3.adj1> st.minw.2016[state,],
                              w3.adj1,
                              st.minw.2016[state,]) )

#to be done: adjust some states by inflation.
```  

## Get the $N$   

### Identify the relevant universe  

```{r, echo=FALSE, }
temp.working.age.pop <- round( sum(df$final_weights, na.rm = TRUE)/1e6, 1)
temp.employed.pop <- round( sum( df$final_weights * (df$lfstat == "Employed"), na.rm = TRUE)/1e6, 1)
temp.unemployed.pop <- round( sum( df$final_weights * (df$lfstat == "Unemployed"), na.rm = TRUE)/1e6, 1)
temp.nilf.pop <-  round( sum( df$final_weights * (df$lfstat == "NILF"), na.rm = TRUE)/1e6, 1)
temp.salary <- round( with(df, sum(final_weights * (empl == 1 &  (selfinc == 0 & selfemp == 0)), na.rm = TRUE))/1e6 , 1)
temp.salary.g1 <- round(with(df, sum(final_weights * ( (empl == 1 & selfinc == 0 & selfemp == 0) & (paidhre == 1 | hrsvary != 1 | is.na(hrsvary) ) & (wage3==0 | is.na(wage3) ) ) , na.rm = TRUE))/1e6, 1)
temp.nohour <- round(with(df,  sum(final_weights * (empl == 1 & (selfinc == 0 & selfemp == 0) & (paidhre == 0 |is.na(paidhre))), na.rm = TRUE))/1e6, 1)
temp.nohour.hours.vary <- round(with(df, sum( final_weights * (empl == 1 & (selfinc == 0 & selfemp == 0) & (paidhre == 0 | is.na(paidhre) ) & hrsvary == 1), na.rm = TRUE))/1e6, 1)
temp.pop.of.interest <- round(with(df, sum(final_weights * (empl == 1 & (selfinc ==0 & selfemp == 0) & ( (paidhre == 0 & hrsvary != 1) | paidhre ==1 )  & wage3 != 0) , na.rm = TRUE))/1e6, 1)
```

According to the CPS data the population of working age in 2013 was `r temp.working.age.pop` million*. Of those, `r temp.employed.pop` million were working, `r temp.unemployed.pop`, were unemployed and `r temp.nilf.pop` were not in the labor force (NILF). 

Among those employed, `r temp.salary` million workers receive a salary (not self employed or self incorporated).  A small number of salary workers (`r temp.salary.g1` million) did not reported any wages and were excluded from the sample. Of the employed salary workers `r temp.nohour` million did not report an hourly wage and their wage was extrapolated from their reported paid period in to a per-hour basis. However, `r temp.nohour.hours.vary` million workers from this group reported having varying hours. Their wages were not calculated and were also excluded from the sample.  As a result the final number of workers where a rise in the minimum wage can have a direct effect is `r temp.pop.of.interest` million (= `r temp.salary` - `r temp.salary.g1` -  `r temp.nohour.hours.vary`), this is our universe of interest. Figure 1 presents visual representation of all these populations. 

*[FH: why are some indiv with age >65?]  
```{r desc stats2,  eval=TRUE, echo=FALSE, warning=FALSE, message=FALSE}
# Tag population of interest
pop_of_int <- with(df,
                   (empl == 1 &
                      (selfinc == 0 & selfemp == 0) & 
                      ( (paidhre == 0 & ( hrsvary != 1 | is.na(hrsvary) )  )  | paidhre == 1 )  & 
                      !(wage3 == 0 | is.na(wage3) ) )
  )

# To compute the new total of workers we multiply the original weigths by the growth rate. 
table_1  <- df  %>% 
   summarise("(1) Total" =
               sum(final_weights, na.rm = TRUE), 
             "(2) Employed" = 
               sum( final_weights * (empl == 1), na.rm = TRUE), 
             "(3) Salary (among employed)" = 
               sum(final_weights * (empl == 1 &      #Salary worker if 
               (selfinc == 0 & selfemp == 0))        #not self employed or     
                , na.rm = TRUE),                     #self incorp.

             "(4) Not Paid hourly (among salary)" = 
               sum(final_weights * (empl == 1 &       # Not paid hourly if salary and 
               (selfinc == 0 & selfemp == 0) &        # not paid hourly
               (paidhre == 0 | is.na(paidhre) )), na.rm = TRUE), 

             "(5) Hours Vary (among not paid hourly)" =
               sum(final_weights * (empl == 1 &       #Hours vary if not paid hourly and 
               (selfinc == 0 & selfemp == 0) &        #hours vary
               (paidhre == 0 | is.na(paidhre) ) & hrsvary == 1), na.rm = TRUE),

             "(6) No wage (in (3) but not in (5))" = 
               sum(final_weights * ( (empl == 1 & selfinc == 0 & selfemp == 0) & 
                   (paidhre == 1 | hrsvary != 1 | is.na(hrsvary) ) & 
                   (wage3==0 | is.na(wage3) ) ) , na.rm = TRUE), 
             
             "Population of Interest = (3) - (5) - (6)" = 
              sum(final_weights * (empl == 1 & (selfinc ==0 & selfemp == 0) & 
                    ( (paidhre == 0 & hrsvary != 1) | paidhre ==1 )  & 
                    wage3 != 0) , na.rm = TRUE)
             )

table_1 <- t(table_1)
colnames(table_1) <- "N"
table_1 <- format(table_1, big.mark = ",", digits = 0, scientific = FALSE)


table_1_uw  <- df  %>% 
   summarise("(1) Total" =
               sum(!is.na(final_weights), na.rm = TRUE), 
             "(2) Employed" = 
               sum( 1 * (empl == 1), na.rm = TRUE), 
             "(3) Salary (among employed)" = 
               sum( 1 * (empl == 1 &             #Salary worker if 
               (selfinc == 0 & selfemp == 0))        #not self employed or     
                , na.rm = TRUE),                     #self incorp.

             "(4) Not Paid hourly (among salary)" = 
               sum( 1 * (empl == 1 &              #Not paid hourly if salary and 
               (selfinc == 0 & selfemp == 0) &        # not paid hourly
               (paidhre == 0 | is.na(paidhre) )), na.rm = TRUE), 

             "(5) Hours Vary (among not paid hourly)" =
               sum( 1 * (empl == 1 &              #Hours vary if not paid hourly and 
               (selfinc == 0 & selfemp == 0) &        #hours vary
               (paidhre == 0 | is.na(paidhre) ) & hrsvary == 1), na.rm = TRUE),

             "(6) No wage (in (3) but not in (5))" = 
               sum( 1 * ( (empl == 1 & selfinc == 0 & selfemp == 0) & 
                   (paidhre == 1 | hrsvary != 1 | is.na(hrsvary) ) & 
                   (wage3==0 | is.na(wage3) ) ) , na.rm = TRUE), 
             
             "Population of Interest = (3) - (5) - (6)" = 
              sum( 1 * (empl == 1 & (selfinc ==0 & selfemp == 0) & 
                    ( (paidhre == 0 & hrsvary != 1) | paidhre ==1 )  & 
                    wage3 != 0) , na.rm = TRUE)
             )

table_1_uw <- t(table_1_uw)
colnames(table_1_uw) <- "N_unweighted"
table_1_uw <- format(table_1_uw, big.mark = ",", digits = 0, scientific = FALSE)

table_1 <- cbind(table_1, table_1_uw)

new_total_n <- format(sum(df$final_weights[pop_of_int==1] * 
                            (1 + workers.gr)^3, 
                          na.rm = TRUE), big.mark=",")

#Summary stats of wage
sum.stas1 <- function(x, wt) {
   c( "mean" = weighted.mean(x,w = wt, na.rm = TRUE),
      "sd" = sqrt( wtd.var(x, weights = wt) ) , 
      "median" = wtd.quantile( x, weights = wt, prob = c(.5)) ,
      wtd.quantile( x, weights = wt, prob = c(.1, .9) ) )
} 

table_2 <- df %>% 
  filter(pop_of_int == 1 & !is.na(wage3)) %>% 
    with(sum.stas1(wage3, final_weights))

table_2 <- cbind(table_2)
colnames(table_2) <- "Wage"


table_3 <- df %>% 
  filter(pop_of_int == 1 & !is.na(wage3)) %>% 
    summarise("> $7.5" = weighted.mean(wage3<7.5,w = final_weights), 
              "> $9" = weighted.mean(wage3<9,w = final_weights), 
              "> $10.10" = weighted.mean(wage3<10.10,w = final_weights), 
              "> $13" = weighted.mean(wage3<13,w = final_weights), 
              "> $15" = weighted.mean(wage3<15,w = final_weights) 
              )

table_3 <- t(table_3)
colnames(table_3) <- "Perc"


table_4 <- matrix(NA, 7, 2)
colnames(table_4)  <- c("2013", "2016: status quo")
rownames(table_4) <- c("Salary workers", 
                       "Median wage", 
                       "% < 7.5","% < 9", 
                       "% < 10.10", "% < 13", 
                       "% < 15" )

table_4[1,1] <- table_1[7]
table_4[1,2] <- new_total_n
table_4[2,1] <- table_2[3]

table_4[2,2] <- round( with(df[pop_of_int == 1 & !is.na(df$w3.adj.min), ], 
                            wtd.quantile( w3.adj.min, weights = final_weights * 
                            (1 + workers.gr)^3, prob = c(.5) ) ), digits = 2 )

table_4[3:7,1]  <- round(as.matrix(table_3), digits = 2)

aux.1 <- df %>% 
  filter(pop_of_int == 1 & !is.na(w3.adj.min)) %>% 
    summarise("> $7.50" = weighted.mean(w3.adj.min<7.5,w = final_weights * (1 + workers.gr)^3), 
              "> $9" = weighted.mean(w3.adj.min<9,w = final_weights * (1 + workers.gr)^3), 
              "> $10.10" = weighted.mean(w3.adj.min<10.10,w = final_weights * (1 + workers.gr)^3), 
              "> $13" = weighted.mean(w3.adj.min<13,w = final_weights * (1 + workers.gr)^3), 
              "> $15" = weighted.mean(w3.adj.min<15,w = final_weights * (1 + workers.gr)^3) 
              )

table_4[3:7,2] <- round( as.matrix(aux.1), digits = 2 )



```  


```{r Universe, eval=TRUE,echo=TRUE, warning=FALSE, message=FALSE, results="hide", error=FALSE, collapse=TRUE, fig.show='hold'}

#if (!(length(dev.list()) == 0)) { dev.off() }
#x11()
universe.1 <- df %>% 
        mutate("teen" = ifelse(age<20, "teen", "adult"), 
               "selfemp_inc" = 1 * (selfemp == 1 | selfinc == 1), 
               "pop_of_int" = 1 * pop_of_int) %>%  
        group_by(lfstat,selfemp_inc, teen, pop_of_int)  %>% 
        summarise("total" = sum(final_weights, na.rm = TRUE))

universe.1$selfemp_inc[universe.1$lfstat!="Employed"] = NA 
universe.1[universe.1$lfstat!="Employed" | universe.1$selfemp_inc==1, c("teen", "pop_of_int")] = NA
universe.1$selfemp_inc[universe.1$selfemp_inc==0]  <- "salary"
universe.1$selfemp_inc[universe.1$selfemp_inc==1]  <- "self employed or self incorporated"
#universe.1$pop_of_int <- with(universe.1, ifelse(pop_of_int==1,"included", "excluded"))


treemap(universe.1,
 index=c("lfstat", "selfemp_inc", "teen"),
 vSize=c("total"),
 range = c(7, 15),
 type="index",
 algorithm="pivotSize",
 fontsize.labels = c(12:8),
 border.col = c("#FFFFFF", "#000000","#000000"), 
 aspRatio= 1.5, 
 palette = c("#D3D3D3"), 
title.legend="number of employees",
fontface.labels  = c(3,2,1), 
align.labels=list(c("left", "top"), c("right", "top"), c("right", "bottom") ), 
bg.labels = 1, 
title = "Figure 1: Distribution of population of working age in 2013"
)

```

For the universe of interest, we describe the distribution of hourly wages in 2013 and the forecasted values for 2016. Figure 2.    
  
```{r Dist wage, eval=TRUE,echo=FALSE, warning=FALSE, message=FALSE, results="hide", error=FALSE, collapse=TRUE, fig.show='hold'}
p <-    df  %>% 
  filter(pop_of_int==1 & wage3<=200)  %>% 
  select(wage3, w3.adj.min, final_weights) %>%
  melt(value.variables=c("wage3", "w3.adj.min") , id="final_weights") %>% 
  mutate("final_weights" = ifelse(variable=="w3.adj.min", 
                                  final_weights * gr.factor("workers", 2014, 2016) , 
                                  final_weights) ) %>% 
  ggplot() + 
    geom_density(aes(x = value, 
                     fill=variable, 
                     weight = final_weights, 
                     alpha = 1/2, 
                     colour=variable), bw=1, kernel = "gau") + 
    geom_vline(xintercept = c(7.25, 10.10, 11.5), col="blue") +
    coord_cartesian(xlim = c(0,20)) +   
    guides(alpha = "none", colour="none") + 
    labs(y = NULL, 
         x = "Wage" , 
         title = "Figure 2: Distribution of wages in 2013 and 2016(forecast)")+
    theme(axis.ticks = element_blank(), axis.text.y = element_blank()) +
    theme(legend.justification=c(0,1), 
          legend.position=c(0,1), 
          legend.background = element_rect(fill = "transparent", 
                                           colour = "transparent") )+
    scale_fill_discrete(name=NULL,
                         labels=c("2013", "2016 (Forecast)"))
  print(p)
```

Table 1 below presents more detail statistics for the wage distributions for 2013 and for the forecasted wages of 2016.  
  
```{r, eval=TRUE,echo=FALSE, warning=FALSE, message=FALSE, error=FALSE, collapse=TRUE, fig.show='hold'}
knitr::kable(table_4, caption="Comparison of 2013 and 2016 under the status quo", digits = 1)

```  

  
Among the population of interest, the employement effects of the minimum wage will be computed separatedley for adults ($age\geq 20$) and teenagers ($16 \leq age < 20$). For this purpose we present the wage distribution for both groups. Figure 3. 

```{r Dist wage by age group, eval=TRUE,echo=FALSE, warning=FALSE, message=FALSE, results="hide", error=FALSE, collapse=TRUE, fig.show='hold'}
#####################
#if (!(length(dev.list()) == 0)) { dev.off() }
#x11()
universe.1 <- df %>% 
        mutate("teen" = ifelse(age<20, "teen", "adult"), 
               "wage_c" = cut(wage3, c(-Inf, 7:15,Inf) ), 
               "selfemp_inc" = 1 * (selfemp == 1 | selfinc == 1) ) %>% 
          group_by(lfstat,selfemp, selfemp_inc, teen, wage_c)  %>% 
            summarise("total" = sum(final_weights, na.rm = TRUE)) 

universe.1$selfemp_inc[universe.1$lfstat!="Employed"] = NA
universe.1$teen[universe.1$lfstat!="Employed"] = NA
universe.1$wage_c[universe.1$lfstat!="Employed"] = NA


universe.1$selfemp_inc[universe.1$selfemp_inc==0]  <- "salary"
universe.1$selfemp_inc[universe.1$selfemp_inc==1]  <- "self employed or self incorporated"

universe.1$wage_n <- as.numeric(universe.1$wage_c)

universe.1$color <-heat.colors( nlevels(universe.1$wage_c) )[universe.1$wage_c]
universe.1$color[is.na(universe.1$color)] <- "#D3D3D3"

treemap(universe.1,
 index=c("lfstat", "selfemp_inc", "teen", "wage_c"),
 vSize=c("total"),
 vColor = c("color"),
 range = c(7, 15),
 type="color",
 aspRatio= 1.5, 
 algorithm="pivotSize",
 border.col = c("#FFFFFF", "#000000","#000000","#000000"), 
 sortID="-wage_n", 
 fontsize.labels = c(12:8),
 #aspRatio= c(4,3), 
 palette = c(rev(heat.colors( nlevels(universe.1$wage_c) )),"#D3D3D3"), 
title.legend="number of employees",
align.labels=list(c("left", "top"), c("right", "top"), c("right", "bottom"), c("center", "center")), 
fontface.labels  = c(3,2,1,1), 
bg.labels = 1, 
title = "Figure 3: Distribution of wages among  adults and teenagers"
)

  
#Other ways to plot it:
#  if (FALSE) {
#   df %>% ggplot(aes(x = value)) + geom_density(aes(x = value, fill=variable))  %>% print()  + guides(fill="none")
#   ggplot() + + geom_density(aes(x = value,
#      colour = variable)) + labs(x = NULL) +
#      opts(legend.position = "none") + opts(title = "Densities from a kernel density estimator")
# 
#  df1 <-  iris %>% 
#     select(Sepal.Width, Sepal.Length) %>% 
#     melt()
#  p <- ggplot(df1,aes(x = value)) + geom_density(aes(x = value, fill=variable))
#  print(p)
#  
#  p <-  iris %>% select(Sepal.Width, Sepal.Length) %>% melt() %>% ggplot(aes(x = value)) + geom_density(aes(x = value, fill=variable))
#     print(p)  
# #  ggplot(aes(value, colour = variable)) + 
#     coord_cartesian(xlim = c(0,30), ylim = c(0 , 1e7)) +
#     geom_density(aes(weights=final_weights), bw = 1) +
# ggplot(aes(w3.adj.min)) + 
#   coord_cartesian(xlim = c(0,20), ylim = c(0 , 1e7)) +
#   geom_density(aes(weights=final_weights),
#                 color="red", fill="red") 
# 
#   df %>%     
#     filter(pop_of_int==1 & wage3<=200)  %>% 
#       with( plot(density(wage3, weight =  final_weights),
#                  col=rgb(1,0,0,0.4),
#                  main = "Distribution of Hourly Wages Below $20", 
#                  xlab = "Hourly Wage",  xlim = c(0, 20)))
#     
#   df %>%     
#     filter(pop_of_int==1 & wage3<=200)  %>% 
#       with( lines(density(w3.adj.min, weight =  final_weights), 
#                   col=rgb(1,0,0,0.4),
#                   main = "Distribution of Hourly Wages Below $20", 
#                   xlab = "Hourly Wage", xlim = c(0, 20)))
# 
#   
# 
# # Histogram of wage below $20
# df  %>% 
#   filter(pop_of_int==1 & wage3<=20)  %>% 
#     with(wtd.hist(wage3, breaks = 50, 
#                   xlab = "Hourly Wage", 
#                   freq = TRUE, 
#                   col=rgb(1,0,0,0.4),
#                   main = "Distribution of Hourly Wages Below $20",
#                   weight =  final_weights
#                   ))
# df  %>% 
#   filter(pop_of_int==1 & w3.adj.min<=20)  %>% 
#     with(wtd.hist(w3.adj.min, breaks = 50, 
#                   xlab = "Hourly Wage", 
#                   freq = TRUE, 
#                   main = "Distribution of Hourly Wages Below $20", 
#                   col=rgb(0,0,1,0.4), 
#                   weight =  final_weights * gr.factor("workers", 2014, 2016),
#                   add = TRUE) ) 
# 
# abline(v = 7.25, col = "red")
# 
# abline(v = 9, col = "red")
# 
# abline(v = 10.10, col = "red")
# 
# }
  
  
  
  
  
  
  
  
#TO DO: display number of workers below red bars in reactive fashion. Maybe not even a plot: only a slider with min wage and a
#reactive box with the number of workers that would be below it.
 
```   

### Identify relevant population   

Given our universe, the next step is to identify the population that would be actually affected if a raise in the minimum wage takes place.  The two relevant populations to define now are the number of low wage workers ($\widehat{ N_{lowwage} }$) and the number of workers that would earn less than the new minimum wage ($\widehat{ N_{w\leq MW^{1}} }$). CBO defines a low wage as one below \$11.5 dollars per hour in 2016, and the proposed value used for the minimum wage is \$10.10 dollars per hour. From now on we separate each of this group among adults and teenagers following CBO's convention. 

Three adjustments are applied to the population of workers with wages below the new minumum wage:    
  1 - Workers whose earnings are mainly from tips are tagged and a different minimum wage is apply to them (\$2.13).  
  2 - A fraction $\alpha_{1}$ of $\widehat{ N_{w\leq MW^{1}} }$ is deleted to account for non compliers.  
  3 - A fraction $\alpha_{2}$ of $\widehat{ N_{w\leq MW^{1}} }$ is deleted to account for workers not subject to the Fair Labor Standards Act.  

After this three adjustments, perform over the relevant population forecasted for the year 2016, we obtain the final population.  

#### Tipped workers   
[TO BE COMPLETED] Ask CBO which ocupations they used to identify tiped workers and clarify the conceptual need to adjust for this population.    

Apply different minimum wage to workers who receive more than \$30 in tips. This was applied to 11 occupations (such as waiter, bartender, and hairdresser ~10% if low wage workers)
 
Given that we do not know which 11 categories the report makes reference to, and which variable that defines the categories, we will use the variable `peernuot` to identify tipped workers. This variable overestimates the number of tipped workers (13% as opposed to the 10% mentioned in the report) because it also contains the workers paid overtime or commissions. 

Tipped workers with wages below 7.25 is are 1% of the total tipped workers. Non-tipped workers with wages below 7.25 are 1.6% of the total. 

#### Non compliance    

We estimate the *proportion of low workers that earn less than the their state's minimum wage in 2013* as a proxy for non compliers under the new minimum wage in 2016. The orginal reports mention that it comes up to 12% of the low wage population. 

Following the same footnote we will consider salary workers as non compliers only if their wage is strictly less than 25 cents for non-tipped workers or 13 cents for tipped workers. 

##### Code to compute percentage of non compliers 

```{r Estimating non compliance, eval=TRUE, echo=TRUE, warning=FALSE, message=FALSE}
#Percentage of total workers in 2013 that earn less that their states' minimum wage. 

# variable peernuot seems to be the most appropiate variable to indicate wether or nor receives tips
# 1=YES; 2=NO
non.comp.stats <- df %>%
  select(wage3, state, final_weights, peernuot)  %>%
    filter(pop_of_int == 1 & wage3<=11.5)  %>%
      summarise(
        "% of non compliers w/o adj" = 
          wtd.mean(wage3 < state.minw("2013")[state, ], 
                          weights = final_weights), 
        "% of non compliers with adj" = 
          wtd.mean(wage3 + 0.25 * (peernuot == 2) + 
                          0.13 * (peernuot == 1)  < state.minw("2013")[state, ], 
                          weights = final_weights) 
        )

```  


#### Not covered that might benefit   

[TO BE COMPLETED] Ask CBO how they identify non-FLSA eligibility.   

 - Include not covered by FLSA but expected to be affected: employees of small firms, occupations generally exempt from FLSA, and teenagers in first 90 days of employment.  

```{r Target population, eval=TRUE,echo=TRUE, warning=FALSE, message=FALSE,  error=FALSE, collapse=TRUE, fig.show='hold'}
library(treemap)

if (!(length(dev.list()) == 0)) { dev.off() }
#x11()
asd <- df %>% 
        mutate("teen" = ifelse(age<20, "teen", "adult"), 
               "wage_c" = ifelse(w3.adj.min<11.5, "Low Wage", "High wage"), 
                "min_wage" = ifelse(w3.adj.min<10.10, "Target Pop", "Think label"), 
               "selfemp_inc" = 1 * (selfemp == 1 | selfinc == 1) ) %>% 
          group_by(lfstat,selfemp, selfemp_inc, teen, wage_c, min_wage)  %>% 
            summarise("total" = sum(final_weights, na.rm = TRUE)) 

asd$selfemp_inc[asd$lfstat!="Employed"] = NA
asd$teen[asd$lfstat!="Employed" | asd$selfemp_inc == 1] = NA
asd$wage_c[asd$lfstat!="Employed"] = NA
asd$min_wage[asd$lfstat!="Employed" | asd$wage_c == "High wage" ] = NA



asd$selfemp_inc[asd$selfemp_inc==0]  <- "salary"
asd$selfemp_inc[asd$selfemp_inc==1]  <- "self employed or self incorporated"

asd$wage_n <- as.numeric(asd$wage_c)

asd$color <-heat.colors( nlevels(as.factor(asd$min_wage)) )[as.factor(asd$min_wage)]
asd$color[is.na(asd$color)] <- "#D3D3D3"

treemap(asd,
 index=c("lfstat", "selfemp_inc", "teen", "wage_c", "min_wage"),
 vSize=c("total"),
 vColor = c("color"),
 range = c(7, 15),
 type="color",
  aspRatio= 1.5, 
 algorithm="pivotSize",
  border.col = c("#FFFFFF", "#000000","#000000","#000000"), 
 sortID="-wage_n", 
 fontsize.labels = c(12:8),
 #aspRatio= c(4,3), 
 palette = c(rev(cm.colors( nlevels(asd$wage_c) )),"#D3D3D3"), 
title.legend="number of employees",
align.labels=list(c("left", "top"), c("right", "top"), c("right", "bottom"), c("center", "center")), 
fontface.labels  = c(3,2,1,1), 
bg.labels = 1
)

```   




The estimated percentage of non-compliance is `r paste(format(100*non.comp.stats[2], digits=3),"%", sep="")` of the target population in 2013 (N = `r table_4[1,2]`). 


#### Code to compute percentage of non compliers 

```{r Sim I, echo=FALSE, eval=FALSE}
#need to figure out how to load the data in to the shiny viz

shinyAppDir(
 "apps/app1", 
  options=list(
    width="100%", height=550
  )
)
```  
  
### Summary  
Define $\hat{g(2016|2013)}$ as the growth factor for the population from the year 2013 to 2016 ($\hat{g(2016|2013)} = (1 + \hat{g})^3$, where $\hat{g}$ is the anual growth rate of the population). Then:  
<!--
$$ 
\begin{align}
\widehat{ N^{final}_{teen} } &= \hat{N_{low \, wage}} (2016|2013)  \times P(\hat{w} \leq MW^{1} | \hat{w} \leq  low \, wage) \times
(1 - \hat{\alpha_{1}} - \hat{\alpha_{2}}) \times P(Age \leq 19 | \hat{w} \leq MW^{1}) \\
&= \hat{g(2016|2013)} \times  \hat{N_{employed}} (2013) \times P(\hat{w} \leq low\,wage)  \times P(\hat{w} \leq MW^{1} | \hat{w} \leq  low \, wage) \times \\
&\quad (1 - \hat{\alpha_{1}} - \hat{\alpha_{2}}) \times P(Age \leq 19 | \hat{w} \leq MW^{1}) \\
&= \hat{g(2016|2013)} \times  \hat{N_{employed}} (2013) P(\hat{w} \leq MW^{1} ) \times (1 - \hat{\alpha_{1}} - \hat{\alpha_{2}}) \times P(Age \leq 19 | \hat{w} \leq MW^{1})
\end{align}
$$
-->
$$ 
\begin{align}
\widehat{ N^{teen}_{final} } &= \hat{N^{teen}_{\hat{w} \leq MW^{1} } } (2016|2013) \times 
(1 - \hat{ \alpha^{teen}_{1} } - \hat{ \alpha^{teen}_{2} })\\
&= \hat{ g(2016|2013) } \times  \hat{ N^{teen}_{employed} } (2013) \times P(\hat{w} \leq MW^{1}|teen)  \times (1 - \hat{ \alpha^{teen}_{1} } - \hat{ \alpha^{teen}_{2} })
\end{align}
$$

Analogusly for the adult population: 

$$ 
\begin{align}
\widehat{ N^{adult}_{final} } &= \hat{ g(2016|2013) } \times  \hat{ N^{adult}_{employed} } (2013) \times P(\hat{w} \leq MW^{1}|adult)  \times (1 - \hat{ \alpha^{adult}_{1} } - \hat{ \alpha^{adult}_{2} })
\end{align}
$$


The table below presents the estimate from 2013 for all each component.

```{r compute Ns, eval=TRUE, echo=TRUE, warning=FALSE, message=FALSE}
aux.1 <-  bind_rows( 
  df  %>% 
  filter(pop_of_int == 1) %>% 
  mutate("adult" = ifelse(age>=20, "Adult", "Teen") )  %>% 
  group_by(adult)  %>% 
  summarise( "Salary workers ($\\hat{ N_{employed} }$) (millions)" = sum(final_weights)/1e6,
             "Low wage workers ($w \\leq 11.5 p/h$) (millions)" = sum(final_weights * (w3.adj.min <=11.50))/1e6,
             "% Salary below new MW ($P(\\hat{w} \\leq MW^{1})$)" = wtd.mean( 1*(w3.adj.min <=10.10), 
                                       na.rm = TRUE, weights = final_weights) * 100),   
  df %>% 
  filter(pop_of_int == 1) %>% 
  summarise( "Salary workers ($\\hat{ N_{employed} }$) (millions)" = sum(final_weights)/1e6,
              "Low wage workers ($w \\leq 11.5 p/h$) (millions)" = sum(final_weights * (w3.adj.min <=11.50))/1e6,
              "% Salary below new MW ($P(\\hat{w} \\leq MW^{1})$)" = wtd.mean( 1*(w3.adj.min <=10.10), 
                                      na.rm = TRUE, weights = final_weights) * 100) %>% 
  mutate(adult = "Total" )
)

#Non compliance (starting from a different denominator: 'pop_of_int == 1 & wage3 < 11.5')
aux.2 <- bind_rows(
  df %>%
  select(wage3, age, state, final_weights, peernuot)  %>%
  filter(pop_of_int == 1 & wage3 <= 11.5) %>% 
  mutate("adult" = ifelse(age>=20, "Adult", "Teen") )  %>% 
  group_by(adult)  %>% 
      summarise(
          "% of non compliers ($\\alpha_{1}$)" = 
          wtd.mean(1 * (wage3 + 0.25 * (peernuot == 2) + 
                          0.13 * (peernuot == 1)  < state.minw("2013")[state, ] ), 
                          weights = final_weights)*100 ),  
  df %>%
  select(wage3, age, state, final_weights, peernuot)  %>%
  filter(pop_of_int == 1 & wage3 <= 11.5) %>% 
      summarise(
          "% of non compliers ($\\alpha_{1}$)" = 
          wtd.mean(1 * (wage3 + 0.25 * (peernuot == 2) + 
                          0.13 * (peernuot == 1)  < state.minw("2013")[state, ] ), 
                          weights = final_weights)*100 ) %>% 
          mutate("adult" = "Total")
)
stats2 <- rbind(t(aux.1[,-1 ]), t(aux.2[, -1]) ) 
colnames(stats2) <- t(aux.1[,1])
stats2 <- rbind(stats2, "$\\hat{ g(2016|2013) }$" = gr.factor("workers", 2014, 2016) )
aux.total <- apply(stats2, 2, function(x) x[5] * x[1] * (x[3]/100) * (1 - x[4]/100)  )
stats2 <- rbind(stats2, "$\\widehat{ N_{final} }$ (millions)" = aux.total)


# FH: There a small difference in the overall total and the sum of Teens and Adults:
# stats2[6,3] - sum( (stats2[6, 1:2])  )
# I have pinned down the source of the problem to differences in the way % is calculated (for groups relative to the total):
# (stats2[4,3]) - sum( (stats2[4, 1:2])*(stats2[2, 1:2]/sum(stats2[2, 1:2]))  )
knitr::kable(stats2, caption="Characteristics of target population", digits = 2)

``` 


## Get the $\eta \times \Delta w$  

In order to get the elasticity of labor demand that best fits the context analysed here, two steps were required: (i) identify from the literature the best estimate available; (ii) extrapolate that estimate to fit the specific context of this policy analysis. 

### Getting the best estimates from the literature  

It is unclear the precise mechanism used by CBO to choose the estimates for the labor demand elasticity. Later I will assume that it came from a meta-analysis and calibrate the weights of some of the meta-analysis cited in the report to reflect that choice.  

For now we take their estimate as given: -0.1 for the teenager population, with a "likely range" a range from 0 to -0.2 ("likely range" is used throughout the report to estimate the expert judgment that the elasticity will be in that range 2/3 of the time)[^1]. The reasons provided in the report for choosing this figure can be summarize by the folllowing three points:  
  - More weight was given to studies that exploit across state variation (as opposed to over time country level variation).    
  - The final estimate takes in to account publication bias towards highly negative estimates.  
  - The magnitude of the increase (%39) and the fact that would be indexed to inflation going forward, makes it an unusually large increase in the minimum wage.    
With this elements we can write the chosen elasticy from the literature ($\eta_{lit}$) as the product of the original assesment of the literature ($\eta_{lit}^{0}$), a reduction factor for publication bias ($F_{pub.bias}<1$) and a amplification factor for a larger variation in minimum wage ($F_{large.variation}>1$):

$$ 
\begin{align} 
\eta^{teen}_{lit} &= \eta_{lit}^{0} \times F_{pub.bias} \times F_{large.variation} = -0.1
\end{align}
$$ 
  
Additionally, CBO provides two additional caveats that could be added to the analysis:  
 - Ripple effects on employment were assume to be null as the result of two opposing effects: (i) "ripple-wages" would increase unemployment but (ii) substitution of marginally more productive workers for layed off workers below the new minimum wage would decrease unemploytment. CBO assumes these effects roughlty cancel each other.       
 - CBO acknowledges that effects could be larger (in abs value) during recessions but does not predict a recession for 2016. No estimaton is provided of how much larger those effects would be in case of a recession.  
 
### Extrapolating research estimate to current context  
Three adjustments are proposed: (i) extrapolate elasticity estimates for teenager to adults; (ii) rescale elasticy to population affected by the new minimum wage; (iii) adjust elasticities to relflect average wage variation from and increase in the minimum wage.  

#### Extrapolate from teenagers to adult population  

Most of the estimates from the literate are for teenage population. CBO proposes to extrapolate this estimates to the adult population in the following fashion:
$$
\begin{align}
\eta^{adults}_{lit} = \eta^{teens}_{lit} \times F_{extrapolation}
\end{align}
$$  

Where a value $F_{extrapolation} = 1/3$ is choosen to reflect that the demand for adult labor is suspected to be more inelastic than the demand for teen labor. 

#### Re-escale elasticities to the population affected by the minimum wage  

The literature reports the estimated effect for a given population $\eta_{lit}$. This estimate can be seen as the weighted average between the demand elasticities for the directly affected population ($\eta_{w\leq MW}$), with wages below the new minimum, and the non affected ($\eta_{w > MW}$) populations, with wages above the new minimum: 

$$
\begin{align}
\eta^{g}_{lit} =& p^{g}_{w\leq MW} \eta^{g}_{w\leq MW} + (1 - p^{g}_{w\leq MW})\eta^{g}_{w > MW}  \hspace{4em} g = \{teens, \, adults \}
\end{align}
$$ 

The underlying assumption is that $\eta_{w > MW} = 0$. With this the first proposed adjustment becomes: 

$$
\begin{align}
\eta^{g}_{w\leq MW} =& \frac{\eta^{g}_{lit}}{p^{g}_{w\leq MW}}  \hspace{4em} g = \{teens, \, adults \}
\end{align}
$$ 

The fraction of the population with a forecasted income below the new minimum wage ($p^{g}_{w\leq MW}$) is `r paste(round(stats2["% Salary below new MW ($P(\\hat{w} \\leq MW^{1})$)",2], 1),"%", sep="")` for teenagers and `r paste(round(stats2["% Salary below new MW ($P(\\hat{w} \\leq MW^{1})$)",1], 1),"%", sep="")` for adults.  

#### Adjust elasticities to average wage variation  

Given that the percentual variation from the old wage to the new minimum wage varies for different levels of wages, total effect should be computed as $\sum_{b} \% \Delta w_{b} \eta^{g}_{w\leq MW} \times N_{b}$ for $b = 1 \dots B$ wage brackets.

The report approximates this calculation by computing the employment effect for average wage variation across the total population, in age group $g$, affected by the minimum wage:  $\overline{\%\Delta w^{g}} \times \eta^{g}_{w\leq MW} \times \widehat{ N^{final}_{g} }$.  

Finally CBO argues that as the variation changes the elasticity should be rescaled to reflect such variation. With the elasticity resulting in:

$$
\begin{align}
\widetilde{ \eta^{g}_{w\leq MW} } &=  \frac{\eta^{g}_{lit}}{p^{g}_{w\leq MW}} \times \frac{\%\Delta MW}{\overline{\%\Delta w^{g}}} = \eta^{g}_{lit} \times F_{adjs}  \hspace{4em} g = \{teens, \, adults \}
\end{align}
$$

Looking at historical trends in the CPS, CBO estimates that $F_{adjs}$ is 4.5 for both populations[^2].  In the following table we summarize all the elemnts required to compute $\widetilde{ \eta^{g}_{w\leq MW} }$ 

## Other factors  
CBO reasons that a rise in the minimum wage would have effects in aggregate consumption and this in turn would have effects on employment. The overall effect is estimated as an increase in employment between 30,000 and 50,000 jobs ("a few tens of thousands of jobs"). A narrative argument is provided for the mechanisms behind this effect.

The effects on consumption are separated into direct and indirect. 

### Direct effects on consumption  
 - Job loses $\Rightarrow$ reduction in consumption.  
 - Increase wages $\Rightarrow$ increase consumption.  
 - Less profits for business owners and shareholders  $\Rightarrow$ reduction in consumption.  
 - Increase prices  $\Rightarrow$ reduction in consumption.   

Overall the direct effect on consumption is estimated[?] to be positive due to a higher marginal propensity to consume of the low wage individuals relative to high income ones.   

### Indirect effects on consumption  
 - Increase in consumption $\Rightarrow$ Increase investment in the future  $\Rightarrow$ Increase consumption in the future.  
 - Increase prices of low-wage-intensive items $\Rightarrow$ increase demand in other items  $\Rightarrow$ Bottleneck in other items until firms adjust. 
 
Overall the indirect effect on consumption is estimated[?] to be negative. 

### Overall effect on consumption and its effect on employment  
CBO estimates [?] that the net effect on consumption would be positive and that its effect on employment would be between 30,000 and 50,000 additional jobs for 2016. This effects are estimated for the short run only. The methodology is mention to be similar to the one used to asses the American Recovery and Reinvestment Act ([found here]())

### Prevent double counting  
The estimated elasticities in the literature already account for approximately 10% of the effects through consumption, so the final effect of consumption here is multiplied by 0.9 to prevent double counting.   

$$
\begin{align}
\widehat{OF} &=  40,000 \times 0.9
\end{align}
$$

## Computing effects on employment  

Putting all elements together we get: 
$$
\begin{align}
 \widehat{ \Delta E } &= \sum_{g\in\{A,T\}} \left( \widehat{ N^{final}_{g} } \times \widetilde{ \eta^{g}_{w\leq MW} }\times \overline{\%\Delta w^{g}}  \right) - \widehat{OF}
\end{align}
$$  



```{r employment effect, eval=TRUE, echo=TRUE, warning=FALSE, message=FALSE}
eta.lit <- - 0.1
factor.extrap <- 1/3

stats3 <-  df  %>% 
  filter(pop_of_int == 1) %>% 
  mutate("adult" = ifelse(age>=20, "Adult", "Teen") )  %>% 
  group_by(adult)  %>% 
  summarise("$\\overline{\\%\\Delta w}$" = wtd.mean( ifelse(w3.adj.min <=10.10,
                                            (10.10 - w3.adj.min)/w3.adj.min, NA) , 
                                     na.rm = TRUE, weights = final_weights) * 100 ) 

stats3 <- rbind("$\\eta_{lit}$" = c( eta.lit * factor.extrap , eta.lit ), 
                "$\\eta_{w \\leq MW'}$" = c( eta.lit * factor.extrap , eta.lit ) / 
                  (stats2["% Salary below new MW ($P(\\hat{w} \\leq MW^{1})$)",1:2]/100),
                "$F_{adj}$" = c( 4.5, 4.5 ),
                t(stats3[,-1]) )

aux.1 <- apply(stats3, 2, function(x) x[1] * x[3])
stats3 <- rbind(stats3, "$\\widetilde{\\eta_{w\\leq MW}}$"= aux.1)

colnames(stats3) <- c("Adult", "Teen")  

# As described in doc
delta.e1 <- sum( stats2["$\\widehat{ N_{final} }$ (millions)",1:2] *  
                   stats3["$\\widetilde{\\eta_{w\\leq MW}}$",1:2] * 
                   (stats3["$\\overline{\\%\\Delta w}$",]/100) 
                ) - 0.05 * 0.9

# As it should have been computed according to doc
delta.e2 <- sum( gr.factor("workers", 2014, 2016) * 
                   stats2["Salary workers ($\\hat{ N_{employed} }$) (millions)",1:2] *  
                   stats2["% Salary below new MW ($P(\\hat{w} \\leq MW^{1})$)",1:2]/100 * 
                   ( 1 - stats2["% of non compliers ($\\alpha_{1}$)",1:2]/100 - 0) *       
                   (-0.1) * c(1/3, 1)  /(stats2["% Salary below new MW ($P(\\hat{w} \\leq MW^{1})$)",1:2]/100) * 
                   ( 0.39/(stats3["$\\overline{\\%\\Delta w}$",]/100)) *
                   stats3["$\\overline{\\%\\Delta w}$",]/100  
                ) - 0.05 * 0.9 


# "Arbitrary" Sens. Anal to get lower unemp (0.9 * wage in c, 0.37 to 0.5, 1/3 to 1/4)
# delta.e3 <- sum( stats3$N * stats3$`% Empl Below MW`/100 * ( 1 - stats3$`% of non compliers with adj`/100 - 0) *
#        0.9*stats3$`% Mean Wage Inc`/100 * 0.1/(stats3$`% Empl Below MW`/100) *
#        ((0.9*stats3$`% Mean Wage Inc`/100)/0.5) * c(1/4, 1) 
# ) - 0.05 * 0.9

# library(foreign)
# df.ma <- read.dta("C:/Users/fhocesde/Documents/dissertation/meta-analysis/minwage1.dta")
# #x11()
# 
# hist(df.ma$tstatistic, breaks = 300, xlim = c(-8,4))
# 
# abline(v = c(-1.96, -1.6, 1.6, 1.96), col = "red")  

knitr::kable(stats3, caption="Components of Elasticities", digits = 2) 

```    

Using all the components descbribed above we get $\widehat{ \Delta^{-} E } =$ `r round(delta.e1*1000)` thousand jobs. The report however computes $F_{adjs}$ in a different fashion and gets a value of 4.5 (when computing the values of $F_{adjs}$  from the table below - as oppose to using historical values - we get $\widehat{ \Delta^{-} E } =$ `r round(delta.e2*1000)` thousand jobs). 

# Distributional effects   

In the first step towards obtainting the policy estimates presented in the [introduction](#introduction) we concluded with a figure of $\widehat{ \Delta^{-} E } =$ `r round(delta.e1*1000)` thousand jobs lost.  We now two additional key quantities: the wage gain among those who get a rise do to the new minimum wage, and the distribution of the losses that pay for that raise. The effect of both quantities is estimated at the level of family income.  

## Computing Family income    

As the unit of interest now is the family and detailed information on income is needed, CBO performs the distributional analysis using a different data set from the Current Population Survey. Instead of the ORG, the following analysis uses the CPS Annual Social and Economic Supplement (ASEC) of March 2013. This data contains income information for the year 2012.  


### Computing wages in CPS ASEC 2013  

The hourly wage ($w$) was computed as the ratio of yearly earnings ($y$) and the product of usual number of hours worked in a week ($Hour.per.Week$) and the number of weeks worked in a year ($Weeks.per.Year$). The CPS ASEC data set contains three variables for yearly earnings: `incp_all` `incp_ern` `incp_wag` corresponing to all income, earnings and wages respectively. We choose `incp_wag`. 

For this data set, the weights used in our analysis will be `hhwgt`.  

$$
\begin{align}
\hat{w} = \frac{\hat{y}}{\hat{Hours \, per \, Week} \times \hat{Weeks\,per\,Year}}
\end{align}
$$


#### Loading the data  
```{r loading data cps_asec, eval=TRUE,echo=TRUE, warning=FALSE, message=FALSE}
data_use <- "CPER_ASEC"

# Using CEPR ORG data 
if (data_use == "CPER_ASEC") {
# Checking if working directory contains data, download if not. 
  if ( !("cepr_march_2013.dta" %in% dir()) ) {
  	# create name of file to store data
  	tf <- "cepr_march_2013.zip"
  
  	# download the CPS repwgts zipped file to the local computer
  	download.file(url =  "http://ceprdata.org/wp-content/cps/data/cepr_march_2013.zip", tf , mode = "wb" )
  
  	# unzip the file's contents and store the file name within the temporary directory
  	fn <- unzip( zipfile = tf ,"cepr_march_2013.dta",  overwrite = T )
  }
  df <- read.dta("cepr_march_2013.dta")
}

df <- tbl_df(df)
```  


#### Code for computing wages and descriptive stats  

```{r Descript stats1 cps asec, eval=TRUE,echo=TRUE, warning=FALSE, message=FALSE}
# Tag population of interest FH: (1) Need to deal with 598 NA's and (2) clarify that restrictions here are not 
# exactly the same as CPS ORG.  
pop_of_int <- with(df,
                   (empl == 1 &
                      (selfinc == 0 & selfemp == 0) & 
                      !(incp_wag == 0 | is.na(incp_wag) ) )
                    )

#FH: Should I adjust any wage below the min to the min?

# For CPS ASEC I am using the weights hhwgt
table_5  <- df  %>% 
   summarise("(1) Total" =
               sum(hhwgt, na.rm = TRUE), 
             "(2) Employed" = 
               sum( hhwgt * (empl == 1), na.rm = TRUE), 
             "(3) Salary (among employed)" = 
               sum(hhwgt * (empl == 1 &             #Salary worker if 
               (selfinc == 0 & selfemp == 0))       #not self employed or     
                , na.rm = TRUE)                     #self incorp.
            )

table_5 <- t(table_5)
colnames(table_5) <- "N"
table_5 <- format(table_5, big.mark = ",", digits = 0, scientific = FALSE)

#Summary stats of wage
sum.stas1 <- function(x, wt) {
   c( "mean" = weighted.mean(x,w = wt, na.rm = TRUE),
      "sd" = sqrt( wtd.var(x, weights = wt) ) , 
      "median" = wtd.quantile( x, weights = wt, prob = c(.5)) ,
                 wtd.quantile( x, weights = wt, prob = c(.1, .9) ) )
} 

table_6 <- df %>% 
  filter(pop_of_int == 1 & !is.na(hhwgt)) %>%
    select(incp_wag, hrslyr, wkslyr, hhwgt) 

table_6 <- sapply(table_6[,c("incp_wag", "hrslyr", "wkslyr")],function(x) sum.stas1(x, table_6$hhwgt) )
table_6 <- cbind(table_6)
colnames(table_6) <- c("Earnings", "Hours", "weeks") 

# Compute hourly wages, replace negative vales withs 0's
df$wage <- with(df, incp_wag/(hrslyr * wkslyr) )
df$wage[df$wage<0]  <- NA


df <- df %>% 
    mutate("hhwgt.2013" = gr.factor("workers", 2012, 2013) *  hhwgt , 
         "wage.2013" = gr.factor("wages per worker", 2012, 2013)  *  wage)

table_7 <- df %>% 
  filter(pop_of_int == 1 & !is.na(wage)) %>%
    summarise("N" = sum(hhwgt.2013),
              "> $7.5" = weighted.mean(wage.2013<7.5,w = hhwgt.2013), 
              "> $9" = weighted.mean(wage.2013<9,w = hhwgt.2013), 
              "> $10.10" = weighted.mean(wage.2013<10.10,w = hhwgt.2013), 
              "> $13" = weighted.mean(wage.2013<13,w = hhwgt.2013), 
              "> $15" = weighted.mean(wage.2013<15,w = hhwgt.2013) 
              ) 

table_7 <- t(table_7) 
colnames(table_7) <- "Perc (2013)"  

``` 

#### Adjusting wages 1 
As with the CPS ORG, an adjusment for wages is applied. Unlike the previuos modification, where the adjustments was over a fraction of the population (those who did not report an hourly wage), our understanding is that CBO adjust the wages of all the population in this case.   

The adjustments follows the following formula: 
$$
\begin{align}
w_{ig} &= \alpha w^{raw}_{ig} - (1 - \alpha)  \overline{w^{raw}_{g}} \label{samp.eq1_1} \\
\text{with:    } \quad \overline{w^{raw}_{g}} &= \frac{\sum_{g} w^{raw}_{ig} }{N_{g}} \nonumber \end{align}
$$ 
  
[TO COMPLETE]] Ask CBO about $\alpha$ and $G$ in this case.   

#### Adjusting wages 2  

CBO mentions that the "it found far fewer workers who would be directly affected by the change in the minimum wage than it had in its analysis of employment", using the CPS ASEC we get `r paste(round(table_7[ "> $10.10",]*100, 1), "%", sep="")` workers below the 10.10 threshold, while using the CPS ORG we `r paste(round(table_3[ "> $10.10",]*100, 1), "%", sep="")`. 

We assume that this second adjustment are a linear transformation ("mostly by adjusting some workers wages up to the minimum wage projected to apply to them in 2016 under current law"):
$$
\begin{align}
\widetilde{w_{ig}} &= (1  + I(U \geq 0) \times F_{1}) w_{ig}I(g \in G_{1}) + \\
&\quad w_{ig}( 1- I(g \in G_{1}))  \quad \text{with:  } U\sim Uniform(a,b) 
\end{align}
$$ 

[TO COMPLETE] Ask CBO about this adjustment.  

#### Forecasting wage
The wage forecast is the same methodology as in section 2.  This methodology is applied to a different data set (CPS ASEC) and for one additional year (forcasting from 2012 to 2016) than with the CPS ORG data


##### Code to forecast wages, workers
```{r forecasting cps asec wage, eval=TRUE,echo=TRUE, warning=FALSE, message=FALSE, results = "hide"}
#Wage adjutsment
#CBO mentions that the lowest 10th percent gets a 2.9% growth in anual wage
#I compute the anualized growth rate of wages and creat 10 bins of wage growth
#starting at 2.4%, then adjust by minimum wages of 2016 and get a anualized 
#growth of 2.9% for the lowest decile. 
wage.gr <- ( gr.factor("wages per worker", 2013, 2016) )^(1/4) - 1
workers.gr <- ( gr.factor("workers", 2013, 2016) )^(1/4) - 1


half.gap <- wage.gr - 0.024
wage.gr.bins <- seq(0.024, wage.gr + half.gap, length.out = 10)

# CAUTION: DO NOT apply 'ntile()' fn from dplry as is will split ties differently than 'cut()' and results will no
# be comparable to STATA. 
aux.var  <- wtd.quantile(x = df$wage, probs = 1:9/10,weights = df$hhwgt)
df <- df %>%
        mutate( wage.deciles = cut(wage, c(0, aux.var, Inf) , 
                                right = TRUE, include.lowest = TRUE) ,  
                wage.adj1 =  wage * ( 1 + wage.gr.bins[wage.deciles] )^4) 

# To compute the new total of workers we multiply the original weigths by the growth rate. 
new_total_n <- format(sum(df$hhwgt[pop_of_int==1] * 
                            (1 + workers.gr)^4, 
                          na.rm = TRUE), big.mark=",")

# Here we adjust min wages up to their state minimum wage
df$wage.adj.min <- with( df, ifelse(wage.adj1> st.minw.2016[state,],
                              wage.adj1,
                              st.minw.2016[state,]) )
#FROM NOW ON 'wage.adj.min' is the variable that contains the forecasted wage in 2016 under the status quo
```  


```{r desc stats2 in forcast asec wage,  eval=TRUE, echo=FALSE, warning=FALSE, message=FALSE}
# WHY TABLE 4?
table_8 <- matrix(NA, 7, 2)
colnames(table_8)  <- c("2013", "2016: status quo")
rownames(table_8) <- c("Salary workers", 
                       "Median wage", 
                       "% < 7.5","% < 9", 
                       "% < 10.10", "% < 13", 
                       "% < 15" )
# Total in 2013
table_8[1,1] <- table_5[3]
# projected total in 2016
table_8[1,2] <- new_total_n

# Median wage before projections
table_8[2,1] <- round( with(df[pop_of_int == 1 & !is.na(df$wage), ], 
                            wtd.quantile( wage, weights = hhwgt
                                          , prob = c(.5) ) ), digits = 2 )
# Median wage after projections
table_8[2,2] <- round( with(df[pop_of_int == 1 & !is.na(df$wage.adj.min), ], 
                            wtd.quantile( wage.adj.min, weights = hhwgt * 
                            (1 + workers.gr)^4, prob = c(.5) ) ), digits = 2 )
# Wage distribution in 2013
table_8[3:7,1]  <- round(as.matrix(table_7[-1]), digits = 2)

aux.1 <- df %>% 
  filter(pop_of_int == 1 & !is.na(wage.adj.min)) %>% 
    summarise("> $7.50" = weighted.mean(wage.adj.min<7.5,w = hhwgt * (1 + workers.gr)^4), 
              "> $9" = weighted.mean(wage.adj.min<9,w = hhwgt * (1 + workers.gr)^4), 
              "> $10.10" = weighted.mean(wage.adj.min<10.10,w = hhwgt * (1 + workers.gr)^4), 
              "> $13" = weighted.mean(wage.adj.min<13,w = hhwgt * (1 + workers.gr)^4), 
              "> $15" = weighted.mean(wage.adj.min<15,w = hhwgt * (1 + workers.gr)^4) 
              )

table_8[3:7,2] <- round( as.matrix(aux.1), digits = 2 )

# Histogram of wage below $20
p2 <-    df  %>% 
  filter(pop_of_int==1 & wage<=200)  %>% 
  select(wage.2013, wage.adj.min, hhwgt, hhwgt.2013) %>%
  gather(key = variable, value = value, -c(hhwgt.2013, hhwgt) ) %>% 
  mutate("hhwgt" = ifelse(variable=="wage.adj.min", 
                                  hhwgt * gr.factor("workers", 2013, 2016) , 
                                  hhwgt.2013 ) ) %>% 
  ggplot() + 
    geom_density(aes(x = value, 
                     fill=variable, 
                     weight = hhwgt, 
                     alpha = 1/2, 
                     colour=variable), bw=1, kernel = "gau") + 
    geom_vline(xintercept = c(7.25, 10.10, 11.5), col="blue") +
    coord_cartesian(xlim = c(0,20)) +   
    guides(alpha = "none", colour="none") + 
    labs(y = NULL, 
         x = "Wage" , 
         title = "Figure 4: Distribution of wages in 2013 and 2016(forecast)")+
    theme(axis.ticks = element_blank(), axis.text.y = element_blank()) +
    theme(legend.justification=c(0,1), 
          legend.position=c(0,1), 
          legend.background = element_rect(fill = "transparent", 
                                           colour = "transparent") )+
    scale_fill_discrete(name=NULL,
                         labels=c("2013 (Forecast)", "2016 (Forecast)"))
  print(p2) 
knitr::kable(table_8, caption="Comparison of 2013 and 2016 under the status quo", digits = 1)
```  

## Imputing policy effects

### Imputing wage increases  
If the wage is below the proposed new minimum (10.10), we increase that wage up to 10.10 for all the eligible population.  

NEED TO DEFINE BETTER THE ELIGIBLE POPULATION    

#### Ripple effects   
CBO applies an additional wage increase for wages that are in a neibourhood up to 50% of the max increase ($+-0.5(10.10 - 7.25) = +- \$1.4$). Thus, the final imputed wage is:

$$
\tilde{w} =
\begin{cases}
w + 0.5(w - 7.25) \quad if \quad  w \in [8.7, 10.10) \\
w + 0.5(11.5 - w) \quad if \quad  w \in [10.10, 11.5) \\
w \quad o/w
\end{cases}
$$

```{r positive effects,  eval=TRUE,echo=TRUE, warning=FALSE, message=FALSE}
# Get the number of workers whos wage would bebow 10.10 in the status quo (in millions)
N_benes <- sum(df$hhwgt[df$wage.adj.min <= 10.10 & pop_of_int==1], na.rm = TRUE)/1e6
# Tag those workers 
df$below_min <-with(df, ifelse(wage.adj.min <= 10.10 & pop_of_int == 1, 1, 0) )

# Compute total wage increase (yearly in billions) -without ripple effects-
wage.inc <- with(df[df$below_min == 1 & pop_of_int==1, ], 
      sum((10.10 - wage.adj.min) *  hhwgt * hrslyr * wkslyr , na.rm = TRUE) ) / 1e9 

# Create new wage (after inc in min wage)
df$new.wage <- with(df, ifelse(wage.adj.min<10.10 & pop_of_int==1, 
                               10.10, 
                               wage.adj.min) )
# Creates the same but using dplyr
# df <- df %>% mutate( new.wage = ifelse(wage.adj.min>10.10 & wage.adj.min<11.5, 
#                                      10.10 + 0.5 * (10.10 - wage.adj.min), 
#                                      new.wage) )  %>%
#                 mutate( new.wage = ifelse(wage.adj.min>8.7 & wage.adj.min<10.10, 
#                                      10.10 + 0.5 * (wage.adj.min - 7.25), 
#                                      new.wage) )
df$new.wage <- with(df, ifelse(wage.adj.min>10.10 & wage.adj.min<11.5 & pop_of_int==1, 
                               10.10 + 0.5 * (11.5 - wage.adj.min),
                               ifelse(wage.adj.min>8.7 & wage.adj.min<10.10 & pop_of_int==1,
                                        10.10 + 0.5 * (wage.adj.min - 7.25),
                                       new.wage
                                        ) ) )
# Total gain with ripple effects but without destroying any jobs  
wage.inc.with.ripple <- df %>% 
    #filter(below_min == 0)  %>% 
      with( sum((new.wage - wage.adj.min) *  hhwgt * hrslyr * wkslyr , na.rm = TRUE) ) / 1e9 



``` 

The total number of workers that are potentially eligible for a raise, before destroying jobs, is `r round(N_benes, 1)` millions.

### Destroying jobs  
The imputation above so far is applied to all workers below the minimum wage (and the ripple effects). Now we need to remove the $\widehat{\Delta E} = `r round(delta.e2, 2)`$  workers by imputing them a wage of 0. CBO chose to not move the wage all the way to 0 but to cut it in half and apply such imputation to $2\widehat{\Delta E}$. When destroying jobs CBO argued that the effect would be heavier on teenagers and low wage adutls. We implement this in the follwogin algorithm:

Replace $\tilde{w} = \tilde{w}/2$ if :  
  - $w \in [7.25, 10.10)$    
  - $\frac{exp(m(x))}{1 + exp(m(x))} > Uniform[\theta]$  with $m(x) = \beta_{1} TEEN - \beta_{2} ADULT\times w$ and $\beta_{1}, \beta_2 >0$  
  - Choose $\theta$ to destroy $2\widehat{\Delta E}$ jobs.     
[TO COMPLETE] Check with CBO. So far I am destroying jobs uniformly across workers earning less than 10.10
```{r job destruction, }  
#NOT USING THIS FOR NOW
m_x <- function(beta1 , beta2) with(df, 
                                    beta1 * ( age <= 19 ) - beta2 * ( age >= 20 ) * 
                                      wage.adj.min - 1000 * 
                                      (wage.adj.min> 10.10 | pop_of_int == 0) )

#WOULD LIKE TO MAKE THE NEXT OPTIMIZATION MORE EFFICIENT
num.to.del <- - delta.e1*1e6*2

theta <- 7
cut.job <- 0
while ( abs(sum(cut.job * df$hhwgt , na.rm = TRUE) - num.to.del) >= 1e5)  { 
  set.seed(123)
  cut.job <- 1* ( df$wage.adj.min<=10.10 & pop_of_int==1 & (theta*runif(dim(df)[1], min = 0, max = 1) < 0.5) )
  # sum(cut.job * df$hhwgt , na.rm = TRUE)/1e6
  if (sum(cut.job * df$hhwgt , na.rm = TRUE) - num.to.del >= 10000) {
    theta <- theta * 1.001 
  } else{
    theta <- theta * 0.999
  }
   #print(theta)
} 
teen <- df$age<20
prop.table(with(df[pop_of_int == 1, ], table(cut.job,teen)), 2)

# df$new.wage <- df %>% transmute(new.wage = ifelse(cut.job ==1 , new.wage/2, new.wage) )
df$cut.wage <- with(df, ifelse(cut.job == 1, wage.adj.min/2, wage.adj.min))
df$new.wage <- with(df, ifelse(cut.job == 1, wage.adj.min/2, new.wage))


# Compute total wage increase after ripple effects (yearly in billions)
wage.gain.total <- df %>% 
      with( sum( (new.wage - wage.adj.min) * 
                   hhwgt * hrslyr * wkslyr , na.rm = TRUE) )/ 1e9 

wage.loss.total <- with(df, 
                        sum( (wage.adj.min - cut.wage) * 
                                   hhwgt * (hrslyr * wkslyr) , na.rm = TRUE ) )/1e9
```  

### Other income losses.   
Income losses from a reduction in profits ($\Delta^{-}\pi$) and an increase in agregate prices ($\Delta^{+}P$) is estimated[?] to be distributed as following: 1% of the losses for those below the poverty line (PL), 29% for those between 1 and 6 PL, and 70% for those above 6PL. 


## Computing family income under status quo and minimum wage increase  

The family income forecast was computed as the sum of forecasted wages and non-wage income:
$$
\begin{align}
\widehat{Y_{h}(2016|2013)} &= \sum_{i \in h} \left( g_{w}\hat{w} + \sum_{l}(g_{nw_{l}}\hat{nw_{l}}) \right) 
\end{align}
$$ 

 The other components of family income were forecasted as follows: when a growth rate was available for the subcomponent it was applied (the only one mentioned is interest and dividends), otherwise the growth rate was equal to the change in the price index for personal consumption 
 
 [TO BE COMPLETED] Ask CBO how do they decompose the income. 

### Growth of non working population  
Forecasts of population growth were the same for working population (NEED TO ALSO APPLY FOR ALL WORKERS). For non working population, the growth rate was matched to CBO forecasts for that group. 

```{r income, eval=TRUE }
#FOR THE FIRST TIME WE NOW LOOK AT OVERALL INCOME AND THE WHOLE POPULATION

#FH:again, I would rather use the non wage growth
non.wage.gr <- ( gr.factor("Price Index, Personal Consumption", 2013, 2016) )^(1/4) - 1


#Adjust non-wage income  
#Separate HH income in to wage and non-wage
df$non.wage <- (df$incp_ern - df$incp_wag) 
df$non.wage.adj1 <- df$non.wage * gr.factor("Price Index, Personal Consumption", 2013, 2016)


# Family wages under status quo 
df$sq.wage <- with(df, wage.adj.min * (hrslyr * wkslyr))
df <- df %>% 
  group_by(hhseq) %>% 
  mutate(fam.sq.wage = sum(sq.wage, na.rm = TRUE))
# Famility wages with wage loss
df$cut.wage.total <- with(df,cut.wage * (hrslyr * wkslyr))
df <- df %>% 
  group_by(hhseq) %>% 
  mutate(fam.cut.wage = sum(cut.wage.total, na.rm = TRUE))

# Family wages under new min wage (incl. wage gains and employment loses)
df$new.wage.total <- with(df,new.wage * (hrslyr * wkslyr))
df <- df %>% 
  group_by(hhseq) %>% 
  mutate(fam.new.wage = sum(new.wage.total, na.rm = TRUE))

# Income 
df$fam.sq.income <- with(df, fam.sq.wage + non.wage.adj1)
df$fam.cut.income <- with(df, fam.cut.wage + non.wage.adj1)
df$fam.new.income <- with(df, fam.new.wage + non.wage.adj1)

# get per capita
df <- df %>% 
  group_by(hhseq)  %>% 
  mutate("N.fam"= n(), 
         "new.income.pc" = fam.new.income/N.fam,
         "sq.income.pc" = fam.sq.income/N.fam, 
         "cut.income.pc" = fam.cut.income/N.fam, 
         "fam.wgt" = sum(hhwgt, na.rm = TRUE) )


# Computing differences in income 
df %>% select(fam.new.income,fam.sq.income, perno,hhwgt, N.fam) %>% 
  filter(perno == 1) %>% 
  with(sum( (fam.new.income - fam.sq.income) * hhwgt, na.rm = TRUE) ) /1e9

#AQUI VOY - UNDERSTAN INCOME WELL!!!!!

# Compute variation by hhld
asd <- df %>% select(new.income.pc,sq.income.pc,perno,N.fam, hhwgt) %>% filter(perno == 1) %>% 
  mutate( "variation" = new.income.pc - sq.income.pc ) %>% select(variation, sq.income.pc)

#x11()
with(asd,plot(sq.income.pc, variation, xlim = c(0,2e5), 
              col = rgb(0,0,0,0.1) , pch =19, cex = .2) )
abline(v = 11740*1:6, col = "red", lty = 1, cex=1.3)

inc.quartiles <- with(df[df$perno==1,],
                      wtd.quantile(x = fam.sq.income, probs = c(.25, .5, .75), weights = hhwgt*N.fam))


abline(v = inc.quartiles, col = "blue", lty =3)

losses <- (wage.gain.total - wage.loss.total)*1e9

asd1 <- with(df, findInterval(x = sq.income.pc, vec = c(-Inf,11740, 6*11740, Inf)))

asd2  <- losses * c(0.30, 0.29, 0.41) / wtd.table(asd1, weights = df$hhwgt)[[2]]

asd3 <- asd2[with(asd, findInterval(x = sq.income.pc, vec = c(-Inf,11740, 6*11740, Inf) )) ]

with(asd, points(sq.income.pc ,-asd3,  xlim = c(0,2e5), 
              col = rgb(0,0,1,0.1) , pch =19, cex = .2) )


#Average income per group (height of rectangle)
asd1 <- df %>% group_by(findInterval(sq.income.pc, c(-Inf, 11770*1:6, Inf) )) %>% summarise(log(mean(new.income.pc)))

#number of individuals per group (width of recangle)
asd2 <- df %>%  with( wtd.table(findInterval(sq.income.pc, c(-Inf, 11770*1:6, Inf)), weights = hhwgt/1e6) )

#Create here a kick-ass viz
if (FALSE) {
  widths = asd2$sum.of.weights
  heights = asd1$`log(mean(new.income))`
  
  barplot(heights, widths, space=0, 
          col = rgb(0,0,0,.3))
}

table_9 <- matrix(NA, ncol = 4, nrow = 4)
colnames(table_9)  <- c("<1PL", "[1PL, 3PL)", "[3PL, 6PL)", ">6PL")
rownames(table_9)  <- c("wage gains", "wage loses", "other loses", "N_i")



#Classifying by PLs and using sq.income/N.fam
deciles <- with(df[df$perno==1,],wtd.quantile(x = sq.income.pc, probs = 1:9/10, weights = hhwgt*N.fam))

df$income.group <- with(df, findInterval(x = sq.income.pc, vec = c(-Inf,c(1,3,6)*11740, Inf)))

table_9["N_i", ] <- wtd.table(with(df, income.group) , weights = df$hhwgt)[[2]]/1e6
  
aux.1 <- df %>% group_by(income.group)  %>% summarise(mean((new.income.pc - sq.income.pc)/N.fam, na.rm = TRUE))
table_9[ "wage gains", ] <- t(as.data.frame(aux.1[,2]) )
  
  
#Wage loses: compared to SQ
aux.1 <- df %>% group_by(income.group)  %>% summarise(mean((sq.income.pc - cut.income.pc), na.rm = TRUE))  
table_9[ "wage loses", ] <- t(as.data.frame(aux.1[,2]) )

#Imputing the balnce of the losses
table_9[ "other loses", ] <- losses * c(0.01, rep(.29/2, 2), 0.70)/(table_9[ "N_i", ]*1e6)

#FH: Need to modify/add table that shows number of people affected on each section



#FH: questions for Phil:
#    - what is the precise way to define family in CPS?
#    - what is the standard way to choose one obs per family?
```  

## Other considerations  

### Economywide income effect  

CBO argues that the overall effect on the economy is positive and of \$2 billion dollars for 2016.  

### Quantifying loses  
- Mix gains and loses. 
- Output lost - increase in aggregate demand (!) 
- net gain (!) of 2 billion dollars.  

### Distributional effects   
- Only results, no methodology at all!  This is probably the most important (and overlooked part of the report)

### Interaction with other programs 
No interactions with other programs is assumed (SNAP or EITC).     


### What is in the 2/3  
  - CBO acknowledges uncertainty in the estimates of elasticity due to possible technological changes in the future.  
  - 

# Results 

```{r final results,}

output.template1 <- cbind(output.template1, "Replication" = rep(NA, 7))
output.template2 <- cbind(t(output.template2), "Replication" = rep(NA, 5), "Replication" = rep(NA, 5))

knitr::kable(round(table_9), caption="Summary of effect of raising the minimum wage", digits = 1)
knitr::kable(
  list(
    output.template1,
    output.template2
  ),
  caption = 'Policy estimates in CBO report and Replication Results', booktabs = TRUE, 
  align = 'c'
)
mod.output.template

```  



[^1]: The report presents smaller estimates for the \$9.00 dollar option (-0.075). The rationale is that a smaller increase (in magnitude but also not indexed by inflation, and with later implementation than the 10.10 option)   will allow firms to adjust other margins before reducing employment. 

[^2]: CBO calculated the fraction of teenagers with earnings below the minimum wage from 1979 to 2009 and the result came to about a third. Then they look at the average change in earnings for teenagers subject to the minimum wage over the same period, and compared that to the nominal change in each variation of the minimum wage. This ratio came to be about 1.5. With this the final estimates for the elasticity for teenagers came to be 4.5 ($1.5/(1/3)$) times higher than what is estimated in the literature.   

